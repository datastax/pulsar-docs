:activeTopics: 100
:messageSize: 1MB
:messageThroughput: 5000 messages per second
:retentionPolicy: 1 month
:ttlPolicy: 1 month
:tieredStoragePolicy: n/a
:messageReplicationFactor: 3

= Production Cluster Sizing

This document summarizes DataStax's recommendations for the sizing and optimization of Apache Pulsars cluster on Linux in a production environment.

Of course, the sizing of a cluster depends on factors like use case and expected load, so this document is not intended to be a one-size-fits-all guide. Rather, we'd like to demonstrate how we consider and solve the problems inherent in sizing Pulsar clusters, and assist you on your journey to unlocking the scaling power of Pulsar.

This page summarizes the requirements, assumptions, definitions, and methodologies that inform our cluster sizing recommendations.
For deployment-specific recommendations, see the following pages:

* xref:production-cluster-vm.adoc[]
* xref:production-cluster-multiregion.adoc[]
* xref:production-cluster-sandbox.adoc[]

== Dedicated VMs or Kubernetes?

As you begin your journey to design an Apache Pulsar cluster, one of the first questions to consider is what infrastructure your cluster will run on.
Most of this guide will focus on running a cluster with dedicated virtual machines.
While Kubernetes is the more popular option, it is easier to express disk calculations, throughput, and secure communications in terms of a VM.
We will call out Kubernetes specifics as they arise in the guide.

== Aggregated cluster workload

To size a cluster, you need a general understanding of what workloads will be running.
Obviously, it’s almost impossible to definitively know the exact applications and message sizes that will be used, and if you cluster is successful, more teams will want to use it! So we’ve collected the “building blocks” of sizing a cluster, which we call an “aggregated cluster workload”. Think of it as a loosely calculated algorithm to approximate cluster sizing.

* Average message size (uncompressed) - this is the most important number to understand. A message is sized by the number of bytes. The data that composes a message includes a *message key*, *message properties*, and the *message payload*. A *message key* is roughly the same number of characters as a GUID. *Message properties* is a key/value collection of metadata, so the number of characters varies. The *message payload* accounts for the bulk of the sizing variability. To start, assume the message is a JSON string with some number of characters. +
For more on message compression, see the https://pulsar.apache.org/docs/2.11.x/concepts-messaging/#compression[Pulsar documentation].

* Incoming message throughput (e.g. # of messages per second) - this is the second most important number to understand. Throughput is expressed as a number of messages that the cluster can produce in a second. Think about this number in terms of steady traffic and burst traffic. Pulsar can scale brokers to handle bursts, so you don’t need to size for maximum workload. If you were streaming in data every time someone clicked on a website and the site received a constant 2000 views per second, then your minimum throughput must be able to handle a load above that requirement, because that stream won't be the only load on the cluster.

* Number of active topics - If the applications using the cluster will be event-driven, then there will most likely be quite a few topics, possibly one for every event type. If the cluster is mostly for streaming data, then each data type will have multiple topics supporting it.

* Message retention and TTL period - Assume that most of the topics in the cluster will be persistent, meaning that messages (once acknowledged) are stored for later lookup on disk. The default retention times in Pulsar are XXXX and the default time to live is XXXX. With a feel for average message size and retention policies we can begin to understand storage needs. TTL (time-to-live) determines how long an unacknowledged message lasts before it is deleted, even before it is acknowledged.

* Tiered storage policies - Tiered storage offloads bookkeeper data to cheaper, long-term storage, and can impact cluster sizing. For more on tiered storage, see https://pulsar.apache.org/docs/2.11.x/tiered-storage-overview/[Pulsar documentation].

There are other factors that could be a part of the aggregated cluster workload. As you gain familiarity with Pulsar you can further customize this calculation, but for now, we will estimate with the above numbers to size a cluster.

== Example workload aggregation worksheet
This example cluster will be used for xxxx and xxxx, so we are going to use xxxxx.

.Workload input characteristics
[cols=2*,options=header]
|===
|*Workload input*
|*Value*

| Number of active topics
| {activeTopics}

| Average message size
| {messageSize}

| Incoming message throughput
| {messageThroughput}

| Message retention
| {retentionPolicy}

| TTL Policy
| {ttlPolicy}

| Tiered storage
| {tieredStoragePolicy}

|===

== Pulsar cluster components

Pulsar clusters come in many shapes and sizes. There are minimum components for base functionality, and there are some add-on components that make management and observability easier. For this guide we will focus on the required components and what it takes to make them resilient to outages and highly available in a 3 zone cloud. We will also provide suggestions for add-ons that we use in almost every install.

=== Required components

Zookeeper - this is Pulsar’s meta data store. It keeps data about a cluster’s configuration, it helps the proxy direct messages to a broker, and holds Bookie configuration. To start use 1 instance of Zookeeper in your cluster. Let the # of lookups delegate scaling from there.

* Broker - This is Pulsar's message router.
Ideally, each broker should be fully utilized without becoming a performance bottleneck.
The best way to optimize this is through performance testing based on your cluster's workload characteristics.
Start with 1 broker per cluster and scale as needed.
Fortunately, the Pulsar broker is stateless, so adding or removing brokers has minimal performance impact on the cluster.

* Bookkeeper (bookie) - This is Pulsar’s data store.
Bookkeeper stores message data in a low-latency, resilient way.
Start with 3 bookies distributed across 3 availability zones.
Pulsar uses Bookkeeper’s quorum math to function, so a loss of 1 Bookkeeper instance won’t bring the system down, but will have some data loss.

Auto recovery - This is a Pulsar component that recovers Bookkeeper data in the event of a Bookkeeper outage.

Functions worker(s) - 

[#recommended]
=== Recommended server components

The DataStax Luna Streaming Helm chart deployment includes optional but highly recommended server components for better Pulsar cluster metrics monitoring and operation visibility.
These components are NOT included in the VM-based deployment.
If your enterprise has its own monitoring and metrics dashboarding system, these components are NOT required.

* Pulsar Proxy - This is an optional proxy for routing client requests to brokers.
* Pulsar AdminConsole - This is an optional web-based admin console for managing Pulsar clusters.
* Pulsar Heartbeat - This is an optional component that monitors the health of Pulsar cluster.
* Prometheus/Grafana/Alert manager stack - This is an optional stack for monitoring Pulsar cluster metrics.

== What's next?

See more production-ready sizing examples for the following scenarios:

* xref:production-cluster-vm.adoc[]
* xref:production-cluster-multiregion.adoc[]
* xref:production-cluster-sandbox.adoc[]








